/**
 * ðŸ—£ï¸ Natural Language Processing Engine
 * Xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn cho tiáº¿ng Viá»‡t vÃ  English
 */

class TextProcessor {
    constructor() {
        this.vietnameseStopWords = new Set([
            'lÃ ', 'cá»§a', 'vÃ ', 'cÃ³', 'trong', 'má»™t', 'vá»›i', 'Ä‘Æ°á»£c', 'Ä‘á»ƒ', 'cÃ¡c',
            'cho', 'tá»«', 'nÃ y', 'Ä‘Ã³', 'nhá»¯ng', 'ngÆ°á»i', 'khi', 'vá»', 'nhÆ°',
            'tÃ´i', 'báº¡n', 'chÃºng', 'há»', 'nÃ³', 'mÃ¬nh', 'ta', 'anh', 'chá»‹'
        ]);
        
        this.englishStopWords = new Set([
            'the', 'is', 'at', 'which', 'on', 'a', 'an', 'as', 'are', 'was',
            'were', 'been', 'be', 'have', 'has', 'had', 'do', 'does', 'did',
            'will', 'would', 'should', 'could', 'can', 'may', 'might', 'must'
        ]);

        this.vocabulary = new Map();
        this.wordFrequency = new Map();
        this.bigramFrequency = new Map();
        this.trigramFrequency = new Map();
    }

    // Tokenization - TÃ¡ch tá»«
    tokenize(text) {
        // Xá»­ lÃ½ tiáº¿ng Viá»‡t cÃ³ dáº¥u
        const cleanText = text
            .toLowerCase()
            .replace(/[^\w\sÃ¡Ã áº£Ã£áº¡Äƒáº¯áº±áº³áºµáº·Ã¢áº¥áº§áº©áº«áº­Ä‘Ã©Ã¨áº»áº½áº¹Ãªáº¿á»á»ƒá»…á»‡Ã­Ã¬á»‰Ä©á»‹Ã³Ã²á»Ãµá»Ã´á»‘á»“á»•á»—á»™Æ¡á»›á»á»Ÿá»¡á»£ÃºÃ¹á»§Å©á»¥Æ°á»©á»«á»­á»¯á»±Ã½á»³á»·á»¹á»µ]/g, ' ')
            .replace(/\s+/g, ' ')
            .trim();

        return cleanText.split(' ').filter(token => token.length > 0);
    }

    // Remove stop words
    removeStopWords(tokens, language = 'auto') {
        const stopWords = language === 'vi' ? this.vietnameseStopWords : 
                         language === 'en' ? this.englishStopWords :
                         new Set([...this.vietnameseStopWords, ...this.englishStopWords]);
        
        return tokens.filter(token => !stopWords.has(token));
    }

    // Detect language
    detectLanguage(text) {
        const vietnameseChars = /[Ã¡Ã áº£Ã£áº¡Äƒáº¯áº±áº³áºµáº·Ã¢áº¥áº§áº©áº«áº­Ä‘Ã©Ã¨áº»áº½áº¹Ãªáº¿á»á»ƒá»…á»‡Ã­Ã¬á»‰Ä©á»‹Ã³Ã²á»Ãµá»Ã´á»‘á»“á»•á»—á»™Æ¡á»›á»á»Ÿá»¡á»£ÃºÃ¹á»§Å©á»¥Æ°á»©á»«á»­á»¯á»±Ã½á»³á»·á»¹á»µ]/;
        return vietnameseChars.test(text) ? 'vi' : 'en';
    }

    // Build vocabulary from corpus
    buildVocabulary(texts) {
        console.log('ðŸ“š Äang xÃ¢y dá»±ng tá»« Ä‘iá»ƒn...');
        
        texts.forEach(text => {
            const tokens = this.tokenize(text);
            const language = this.detectLanguage(text);
            const cleanTokens = this.removeStopWords(tokens, language);
            
            // Unigrams
            cleanTokens.forEach(token => {
                if (!this.vocabulary.has(token)) {
                    this.vocabulary.set(token, this.vocabulary.size);
                }
                
                const count = this.wordFrequency.get(token) || 0;
                this.wordFrequency.set(token, count + 1);
            });
            
            // Bigrams
            for (let i = 0; i < cleanTokens.length - 1; i++) {
                const bigram = `${cleanTokens[i]} ${cleanTokens[i + 1]}`;
                const count = this.bigramFrequency.get(bigram) || 0;
                this.bigramFrequency.set(bigram, count + 1);
            }
            
            // Trigrams
            for (let i = 0; i < cleanTokens.length - 2; i++) {
                const trigram = `${cleanTokens[i]} ${cleanTokens[i + 1]} ${cleanTokens[i + 2]}`;
                const count = this.trigramFrequency.get(trigram) || 0;
                this.trigramFrequency.set(trigram, count + 1);
            }
        });
        
        console.log(`âœ… Tá»« Ä‘iá»ƒn Ä‘Ã£ xÃ¢y dá»±ng: ${this.vocabulary.size} tá»«`);
    }

    // Text to vector conversion
    textToVector(text, maxLength = 100) {
        const tokens = this.tokenize(text);
        const language = this.detectLanguage(text);
        const cleanTokens = this.removeStopWords(tokens, language);
        
        const vector = new Array(Math.min(maxLength, this.vocabulary.size)).fill(0);
        
        cleanTokens.forEach(token => {
            const index = this.vocabulary.get(token);
            if (index !== undefined && index < vector.length) {
                vector[index] = 1; // Binary encoding hoáº·c cÃ³ thá»ƒ dÃ¹ng TF-IDF
            }
        });
        
        return vector;
    }

    // TF-IDF calculation
    calculateTFIDF(text, corpus) {
        const tokens = this.removeStopWords(this.tokenize(text));
        const tfidf = {};
        
        // Term Frequency
        const termFreq = {};
        tokens.forEach(token => {
            termFreq[token] = (termFreq[token] || 0) + 1;
        });
        
        // Document Frequency
        const docFreq = {};
        corpus.forEach(doc => {
            const docTokens = new Set(this.removeStopWords(this.tokenize(doc)));
            docTokens.forEach(token => {
                docFreq[token] = (docFreq[token] || 0) + 1;
            });
        });
        
        // Calculate TF-IDF
        Object.keys(termFreq).forEach(token => {
            const tf = termFreq[token] / tokens.length;
            const idf = Math.log(corpus.length / (docFreq[token] || 1));
            tfidf[token] = tf * idf;
        });
        
        return tfidf;
    }

    // Named Entity Recognition - Nháº­n dáº¡ng thá»±c thá»ƒ
    recognizeEntities(text) {
        const entities = [];
        const tokens = this.tokenize(text);
        
        // Patterns for different entity types
        const patterns = {
            person: /^(anh|chá»‹|Ã´ng|bÃ |tháº§y|cÃ´|em|chÃº|cáº­u)\s+([A-Za-zÃ€-á»¹]+)/i,
            location: /^(á»Ÿ|táº¡i|Ä‘áº¿n|tá»«)\s+([A-Za-zÃ€-á»¹\s]+)/i,
            organization: /^(cÃ´ng ty|trÆ°á»ng|Ä‘áº¡i há»c|viá»‡n)\s+([A-Za-zÃ€-á»¹\s]+)/i,
            time: /\d{1,2}:\d{2}|\d{1,2}\/\d{1,2}\/\d{2,4}|hÃ´m nay|ngÃ y mai|hÃ´m qua/i,
            number: /\d+/g,
            email: /\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b/g,
            phone: /\b\d{10,11}\b/g
        };
        
        Object.entries(patterns).forEach(([type, pattern]) => {
            const matches = text.match(pattern);
            if (matches) {
                matches.forEach(match => {
                    entities.push({
                        type,
                        value: match.trim(),
                        position: text.indexOf(match)
                    });
                });
            }
        });
        
        return entities;
    }

    // Part-of-Speech Tagging - GÃ¡n nhÃ£n tá»« loáº¡i
    posTagging(tokens) {
        const posRules = {
            // Danh tá»«
            noun: ['ngÆ°á»i', 'viá»‡c', 'váº­t', 'thá»©', 'cÃ¡i', 'con', 'báº£n', 'cuá»™c'],
            // Äá»™ng tá»«  
            verb: ['lÃ ', 'cÃ³', 'Ä‘Æ°á»£c', 'lÃ m', 'Ä‘i', 'Ä‘áº¿n', 'vá»', 'nÃ³i', 'biáº¿t', 'tháº¥y'],
            // TÃ­nh tá»«
            adjective: ['tá»‘t', 'xáº¥u', 'Ä‘áº¹p', 'to', 'nhá»', 'cao', 'tháº¥p', 'nhanh', 'cháº­m'],
            // Tráº¡ng tá»«
            adverb: ['ráº¥t', 'khÃ¡', 'hÆ¡i', 'cá»±c', 'siÃªu', 'quÃ¡', 'láº¯m', 'nhiá»u', 'Ã­t'],
            // Giá»›i tá»«
            preposition: ['trong', 'ngoÃ i', 'trÃªn', 'dÆ°á»›i', 'vá»', 'cho', 'vá»›i', 'tá»«', 'Ä‘áº¿n']
        };
        
        return tokens.map(token => {
            for (const [pos, words] of Object.entries(posRules)) {
                if (words.includes(token.toLowerCase())) {
                    return { word: token, pos };
                }
            }
            
            // Default rules based on patterns
            if (/\d+/.test(token)) return { word: token, pos: 'number' };
            if (token.length === 1) return { word: token, pos: 'particle' };
            
            return { word: token, pos: 'unknown' };
        });
    }

    // Sentiment Analysis - PhÃ¢n tÃ­ch cáº£m xÃºc
    analyzeSentiment(text) {
        const positiveWords = new Set([
            'tá»‘t', 'hay', 'Ä‘áº¹p', 'tuyá»‡t', 'xuáº¥t sáº¯c', 'hoÃ n háº£o', 'yÃªu', 'thÃ­ch', 
            'vui', 'háº¡nh phÃºc', 'tÃ­ch cá»±c', 'good', 'great', 'excellent', 'amazing',
            'love', 'like', 'happy', 'positive', 'wonderful', 'fantastic'
        ]);
        
        const negativeWords = new Set([
            'xáº¥u', 'tá»‡', 'dá»Ÿ', 'ghÃ©t', 'khÃ´ng thÃ­ch', 'buá»“n', 'tá»©c giáº­n', 'tiÃªu cá»±c',
            'bad', 'terrible', 'awful', 'hate', 'dislike', 'sad', 'angry', 'negative'
        ]);
        
        const tokens = this.removeStopWords(this.tokenize(text));
        let score = 0;
        let positiveCount = 0;
        let negativeCount = 0;
        
        tokens.forEach(token => {
            if (positiveWords.has(token)) {
                score += 1;
                positiveCount++;
            } else if (negativeWords.has(token)) {
                score -= 1;
                negativeCount++;
            }
        });
        
        const sentiment = score > 0 ? 'positive' : score < 0 ? 'negative' : 'neutral';
        const confidence = Math.abs(score) / Math.max(tokens.length, 1);
        
        return {
            sentiment,
            score,
            confidence,
            positiveCount,
            negativeCount,
            totalWords: tokens.length
        };
    }

    // Intent Classification - PhÃ¢n loáº¡i Ã½ Ä‘á»‹nh
    classifyIntent(text) {
        const intents = {
            greeting: ['xin chÃ o', 'chÃ o', 'hello', 'hi', 'hey', 'good morning', 'good evening'],
            question: ['gÃ¬', 'ai', 'Ä‘Ã¢u', 'khi nÃ o', 'táº¡i sao', 'nhÆ° tháº¿ nÃ o', 'what', 'who', 'where', 'when', 'why', 'how'],
            request: ['lÃ m Æ¡n', 'xin hÃ£y', 'cÃ³ thá»ƒ', 'giÃºp', 'please', 'can you', 'could you', 'help me'],
            goodbye: ['táº¡m biá»‡t', 'chÃ o táº¡m biá»‡t', 'bye', 'goodbye', 'see you later'],
            compliment: ['tuyá»‡t vá»i', 'giá»i', 'thÃ´ng minh', 'great job', 'well done', 'excellent'],
            complaint: ['khÃ´ng hÃ i lÃ²ng', 'tá»‡', 'khÃ´ng tá»‘t', 'terrible', 'awful', 'disappointed']
        };
        
        const tokens = this.tokenize(text.toLowerCase());
        const scores = {};
        
        Object.entries(intents).forEach(([intent, keywords]) => {
            let score = 0;
            keywords.forEach(keyword => {
                if (tokens.some(token => token.includes(keyword) || keyword.includes(token))) {
                    score += 1;
                }
            });
            scores[intent] = score;
        });
        
        const maxScore = Math.max(...Object.values(scores));
        const predictedIntent = Object.keys(scores).find(intent => scores[intent] === maxScore);
        
        return {
            intent: maxScore > 0 ? predictedIntent : 'unknown',
            confidence: maxScore / Math.max(tokens.length, 1),
            scores
        };
    }

    // Extract keywords - TrÃ­ch xuáº¥t tá»« khÃ³a
    extractKeywords(text, topN = 5) {
        const tokens = this.removeStopWords(this.tokenize(text));
        const frequency = {};
        
        tokens.forEach(token => {
            if (token.length > 2) { // Bá» qua tá»« quÃ¡ ngáº¯n
                frequency[token] = (frequency[token] || 0) + 1;
            }
        });
        
        const keywords = Object.entries(frequency)
            .sort(([,a], [,b]) => b - a)
            .slice(0, topN)
            .map(([word, freq]) => ({ word, frequency: freq }));
        
        return keywords;
    }

    // Text similarity using Jaccard coefficient
    calculateSimilarity(text1, text2) {
        const tokens1 = new Set(this.removeStopWords(this.tokenize(text1)));
        const tokens2 = new Set(this.removeStopWords(this.tokenize(text2)));
        
        const intersection = new Set([...tokens1].filter(x => tokens2.has(x)));
        const union = new Set([...tokens1, ...tokens2]);
        
        return intersection.size / union.size;
    }

    // Generate n-grams
    generateNGrams(tokens, n = 2) {
        const ngrams = [];
        for (let i = 0; i <= tokens.length - n; i++) {
            ngrams.push(tokens.slice(i, i + n).join(' '));
        }
        return ngrams;
    }

    // Text preprocessing pipeline
    preprocess(text, options = {}) {
        const {
            lowercase = true,
            removeStopwords = true,
            detectLanguage = true,
            extractEntities = false,
            analyzeSentiment = false,
            classifyIntent = false,
            extractKeywords = false
        } = options;
        
        let processedText = text;
        if (lowercase) {
            processedText = processedText.toLowerCase();
        }
        
        const tokens = this.tokenize(processedText);
        const language = detectLanguage ? this.detectLanguage(text) : null;
        const cleanTokens = removeStopwords ? this.removeStopWords(tokens, language) : tokens;
        
        const result = {
            originalText: text,
            processedText,
            tokens: cleanTokens,
            language
        };
        
        if (extractEntities) {
            result.entities = this.recognizeEntities(text);
        }
        
        if (analyzeSentiment) {
            result.sentiment = this.analyzeSentiment(text);
        }
        
        if (classifyIntent) {
            result.intent = this.classifyIntent(text);
        }
        
        if (extractKeywords) {
            result.keywords = this.extractKeywords(text);
        }
        
        return result;
    }

    // Save model data
    save() {
        return {
            vocabulary: Array.from(this.vocabulary.entries()),
            wordFrequency: Array.from(this.wordFrequency.entries()),
            bigramFrequency: Array.from(this.bigramFrequency.entries()),
            trigramFrequency: Array.from(this.trigramFrequency.entries()),
            timestamp: Date.now()
        };
    }

    // Load model data
    load(data) {
        this.vocabulary = new Map(data.vocabulary);
        this.wordFrequency = new Map(data.wordFrequency);
        this.bigramFrequency = new Map(data.bigramFrequency);
        this.trigramFrequency = new Map(data.trigramFrequency);
        
        console.log('âœ… NLP model loaded successfully!');
    }
}

// Language Model for text generation
class LanguageModel {
    constructor(nlpProcessor) {
        this.nlp = nlpProcessor;
        this.ngramModel = new Map();
        this.contextWindow = 3;
    }

    // Build n-gram language model
    buildModel(texts) {
        console.log('ðŸ”¤ Äang xÃ¢y dá»±ng language model...');
        
        texts.forEach(text => {
            const tokens = ['<START>', ...this.nlp.tokenize(text), '<END>'];
            
            for (let i = 0; i < tokens.length - this.contextWindow; i++) {
                const context = tokens.slice(i, i + this.contextWindow).join(' ');
                const nextWord = tokens[i + this.contextWindow];
                
                if (!this.ngramModel.has(context)) {
                    this.ngramModel.set(context, new Map());
                }
                
                const contextMap = this.ngramModel.get(context);
                contextMap.set(nextWord, (contextMap.get(nextWord) || 0) + 1);
            }
        });
        
        console.log('âœ… Language model hoÃ n thÃ nh!');
    }

    // Generate text based on context
    generateText(seedText, maxLength = 50) {
        const tokens = ['<START>', ...this.nlp.tokenize(seedText)];
        const generated = [...tokens];
        
        for (let i = 0; i < maxLength; i++) {
            const context = generated.slice(-this.contextWindow).join(' ');
            const possibleNext = this.ngramModel.get(context);
            
            if (!possibleNext || possibleNext.size === 0) {
                break;
            }
            
            // Weighted random selection
            const choices = Array.from(possibleNext.entries());
            const totalWeight = choices.reduce((sum, [, weight]) => sum + weight, 0);
            
            let random = Math.random() * totalWeight;
            let selectedWord = '<END>';
            
            for (const [word, weight] of choices) {
                random -= weight;
                if (random <= 0) {
                    selectedWord = word;
                    break;
                }
            }
            
            if (selectedWord === '<END>') {
                break;
            }
            
            generated.push(selectedWord);
        }
        
        return generated.slice(tokens.length).join(' ');
    }
}

// Export classes
if (typeof module !== 'undefined' && module.exports) {
    module.exports = {
        TextProcessor,
        LanguageModel
    };
} else {
    // Browser environment
    window.TextProcessor = TextProcessor;
    window.LanguageModel = LanguageModel;
}